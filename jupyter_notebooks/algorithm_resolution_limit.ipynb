{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This calculates the resolution limit of an image analysis algorithm\n",
    "\n",
    "For this example we compare 3D-DAOSTORM in single versus multi-emitter fitting mode, but the approach can of course be used to test the performance of any localization algorithm.\n",
    "\n",
    "Note that the results are not directly comparable with the results of Cohen et al. as among other differences we're using a Gaussian PSF model instead of an Airy PSF model.\n",
    "\n",
    "References:\n",
    "* [Cohen et al, Nature Communications, 2019](https://doi.org/10.1038/s41467-019-08689-x).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the directory\n",
    "Create an empty directory somewhere on your computer and tell Python to go to that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/hbabcock/Data/storm_analysis/jy_testing/\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation parameters\n",
    "\n",
    "These are the parameter values we'll use in the simulation / analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = 10                   # Per pixel background in photons.\n",
    "camera_gain = 1.0\n",
    "camera_offset = 100.0\n",
    "density = 0.017           # Localizations per pixel.\n",
    "im_size = 512             # Image size in pixels.\n",
    "iterations = 20\n",
    "margin = 1\n",
    "model = \"2dfixed\"\n",
    "n_reps = 10               # Number of repeats to average together. 10 or so gives the basic idea\n",
    "                          # but you need hundreds to get curves as smooth as those in Cohen et al.\n",
    "pixel_size = 100.0        # Pixel size in nanometers.\n",
    "roi_size = 10             # Localization ROI in pixels.\n",
    "sigma = 1.5               # Localization sigma in pixels.\n",
    "signal = 1000             # Localization intensity in photons.\n",
    "threshold = 6.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create localizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import storm_analysis.sa_library.sa_h5py as saH5Py\n",
    "\n",
    "def emittersUniformRandom():\n",
    "    # Calculate the size of area covered by the localizations.\n",
    "    size_x = im_size - 2*margin\n",
    "    size_y = im_size - 2*margin\n",
    "\n",
    "    # Calculate number of localizations.\n",
    "    n_locs = int(round(size_x*size_y*density))\n",
    "\n",
    "    # Create localizations.\n",
    "    peaks = {}\n",
    "    peaks[\"sum\"] = signal * numpy.ones(n_locs)\n",
    "    peaks[\"x\"] = margin + size_x * numpy.random.uniform(size = n_locs)\n",
    "    peaks[\"y\"] = margin + size_y * numpy.random.uniform(size = n_locs)\n",
    "    peaks[\"z\"] = numpy.zeros(n_locs)\n",
    "    peaks[\"xsigma\"] = sigma*numpy.ones(n_locs)\n",
    "    peaks[\"ysigma\"] = sigma*numpy.ones(n_locs)\n",
    "    \n",
    "    return peaks\n",
    "    \n",
    "with saH5Py.SAH5Py(\"sim_locs.hdf5\", is_existing = False, overwrite = True) as h5:\n",
    "    h5.setMovieInformation(im_size, im_size, n_reps, \"\")\n",
    "    for i in range(n_reps):\n",
    "        locs = emittersUniformRandom()\n",
    "        h5.addLocalizations(locs, i)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create movie to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.simulator.background as background\n",
    "import storm_analysis.simulator.camera as camera\n",
    "import storm_analysis.simulator.emitters_uniform_random as emittersUniformRandom\n",
    "import storm_analysis.simulator.photophysics as photophysics\n",
    "import storm_analysis.simulator.psf as psf\n",
    "import storm_analysis.simulator.simulate as simulate\n",
    "\n",
    "bg_f = lambda s, x, y, i3 : background.UniformBackground(s, x, y, i3, photons = bg)\n",
    "cam_f = lambda s, x, y, i3 : camera.Ideal(s, x, y, i3, camera_offset)\n",
    "pp_f = lambda s, x, y, i3 : photophysics.Duplicate(s, x, y, i3, \"sim_locs.hdf5\")\n",
    "psf_f = lambda s, x, y, i3 : psf.GaussianPSF(s, x, y, i3, pixel_size)\n",
    "\n",
    "sim = simulate.Simulate(background_factory = bg_f,\n",
    "                        camera_factory = cam_f,\n",
    "                        photophysics_factory = pp_f,\n",
    "                        psf_factory = psf_f,\n",
    "                        x_size = im_size,\n",
    "                        y_size = im_size)\n",
    "    \n",
    "sim.simulate(\"test.tif\", \"sim_locs.hdf5\", n_reps)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze the movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First analyze in single fitter mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import storm_analysis.sa_library.parameters as parameters\n",
    "import storm_analysis.daostorm_3d.mufit_analysis as mfit\n",
    "\n",
    "def createParametersFile(filename):\n",
    "    \"\"\"\n",
    "    Create a 3D-DAOSTORM parameters file.\n",
    "    \"\"\"\n",
    "    params = parameters.ParametersDAO()\n",
    "\n",
    "    params.changeAttr(\"background_sigma\", 8.0)\n",
    "\n",
    "    params.changeAttr(\"camera_gain\", camera_gain)\n",
    "    params.changeAttr(\"camera_offset\", camera_offset)\n",
    "\n",
    "    # Since this parameter can be either a float or an int we need\n",
    "    # to specify which type we want it to be.\n",
    "    params.changeAttr(\"find_max_radius\", 3, node_type = \"float\")\n",
    "\n",
    "    params.changeAttr(\"foreground_sigma\", 1.0)\n",
    "    params.changeAttr(\"iterations\", iterations)\n",
    "    params.changeAttr(\"model\", model)\n",
    "    params.changeAttr(\"pixel_size\", pixel_size)\n",
    "    params.changeAttr(\"roi_size\", roi_size)\n",
    "    params.changeAttr(\"sigma\", sigma)\n",
    "    params.changeAttr(\"threshold\", threshold)\n",
    "\n",
    "    # Don't do tracking.\n",
    "    params.changeAttr(\"radius\", \"0.0\")\n",
    "    params.changeAttr(\"descriptor\", \"1\")\n",
    "\n",
    "    # Don't do drift-correction.\n",
    "    params.changeAttr(\"drift_correction\", 0)\n",
    "\n",
    "    # Don't do z fitting.\n",
    "    params.changeAttr(\"do_zfit\", 0)\n",
    "\n",
    "    params.toXMLFile(filename, pretty = True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only do a single pass of peak finding.\n",
    "iterations = 1\n",
    "\n",
    "# Create an XML file with the current parameters.\n",
    "createParametersFile(\"single_em.xml\")\n",
    "\n",
    "# Analyze the movie.\n",
    "if os.path.exists(\"test_se.hdf5\"):\n",
    "    os.remove(\"test_se.hdf5\")\n",
    "    \n",
    "mfit.analyze(\"test.tif\", \"test_se.hdf5\", \"single_em.xml\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat analysis in multi-fitter mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiples passes of peak finding.\n",
    "iterations = 20\n",
    "\n",
    "# Create an XML file with the current parameters.\n",
    "createParametersFile(\"multi_em.xml\")\n",
    "\n",
    "# Analyze the movie.\n",
    "if os.path.exists(\"test_me.hdf5\"):\n",
    "    os.remove(\"test_me.hdf5\")\n",
    "    \n",
    "mfit.analyze(\"test.tif\", \"test_me.hdf5\", \"multi_em.xml\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate pairwise correlation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.spatial\n",
    "\n",
    "\n",
    "# Average PCF calculation for a movie.\n",
    "def AveragePCF(h5_name):\n",
    "    cnts = 0\n",
    "    r = None\n",
    "    pcf = None\n",
    "    with saH5Py.SAH5Reader(h5_name) as h5:\n",
    "        for [fnum, locs] in h5.localizationsIterator(fields = [\"x\", \"y\"]):\n",
    "            if((fnum%2)==0):\n",
    "                print(\"Calculating PCF for\", fnum)\n",
    "            [r, tmp] = PCF(locs)\n",
    "            if pcf is None:\n",
    "                pcf = tmp\n",
    "            else:\n",
    "                pcf += tmp\n",
    "            cnts += 1.0\n",
    "    print()\n",
    "    pcf = pcf/cnts\n",
    "    return [r, pcf]\n",
    "    \n",
    "    \n",
    "# PCF function calculation.\n",
    "def PCF(locs):\n",
    "    \n",
    "    # Maximum distance for PCF calculation, this should be an integer.\n",
    "    max_dist = 30  \n",
    "\n",
    "    # Create a KDTree to make this more efficient.\n",
    "    kd = scipy.spatial.KDTree(numpy.stack((locs[\"x\"], locs[\"y\"]), axis = -1))\n",
    "    \n",
    "    # Ignore localizations near the edges.\n",
    "    sx = max_dist + 2\n",
    "    ex = im_size - max_dist - 3\n",
    "    mask = (locs[\"x\"] > sx) & (locs[\"x\"] < ex) & (locs[\"y\"] > sx) & (locs[\"y\"] < ex)\n",
    "    \n",
    "    n_bins = 10*max_dist\n",
    "    d_hist = numpy.zeros(n_bins)\n",
    "    for i in range(mask.size):\n",
    "\n",
    "        # Skip localizations that are near the edge.\n",
    "        if not(mask[i]):\n",
    "            continue\n",
    "            \n",
    "        pnt = numpy.array([locs[\"x\"][i], locs[\"y\"][i]])\n",
    "        [dist, index] = kd.query(pnt, k = None, distance_upper_bound = max_dist)\n",
    "        [hist, edges] = numpy.histogram(dist, bins = n_bins, range = (0, max_dist), density = False)\n",
    "        d_hist += hist\n",
    "        \n",
    "    # Normalize.\n",
    "    r = 2.0*numpy.pi*numpy.arange(0.0,max_dist,0.1)*0.1\n",
    "    d_hist = d_hist/numpy.count_nonzero(mask)\n",
    "    d_hist = d_hist[1:]/r[1:]\n",
    "    \n",
    "    density = numpy.count_nonzero(mask)/((ex-sx)*(ex-sx))\n",
    "    d_hist = d_hist/density\n",
    "    \n",
    "    # r in microns.\n",
    "    r = r * 1.0e-3 * pixel_size\n",
    "    \n",
    "    return [r[1:], d_hist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth PCF.\n",
    "[r, gt_pcf] = AveragePCF(\"test_ref.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single fitter PCF. \n",
    "[r, se_pcf] = AveragePCF(\"test_se.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi fitter PCF. \n",
    "[r, me_pcf] = AveragePCF(\"test_me.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as pyplot\n",
    "\n",
    "start = 5\n",
    "pyplot.figure(figsize = (8,6))\n",
    "pyplot.plot(r[start:], gt_pcf[start:], label = \"GT\")\n",
    "pyplot.plot(r[start:], se_pcf[start:], label = \"SE\")\n",
    "pyplot.plot(r[start:], me_pcf[start:], label = \"ME\")\n",
    "pyplot.plot([-0.1,1.3],[1.0,1.0], color = \"gray\")\n",
    "pyplot.legend()\n",
    "pyplot.xlabel(\"r(um)\")\n",
    "pyplot.ylabel(\"g(r)\")\n",
    "pyplot.xlim(0,1.2)\n",
    "pyplot.ylim(0,1.5)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
